# -*- coding: utf-8 -*-
"""618 Group Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PGTdh2lAivl9nUtHHtMvj2JjVcFkDMGb

#***Data Exploration*** using different models to see trends in the attributes given
"""

# Commented out IPython magic to ensure Python compatibility.
#import all the libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

#read all of your data using pandas
#display information about the train data set using info ()
train=pd.read_csv('trainset.csv')
test=pd.read_csv('testset.csv')
train.info()

#we used the heatmap, to see any correlations between attributes
plt.subplots(figsize=(15,15))
sns.heatmap(train.corr(), annot=True)
plt.show()

#we used a seaborn displot to see the density of AGE and distribution of the data
sns.distplot(train['age'], hist=True, kde=True,
             bins=int(180/5), color = 'red',
             hist_kws={'edgecolor':'blue'})

# we used a subplot to see the distrubution of age and to derive insight
fig, ax = plt.subplots()
fig.set_size_inches(35, 15)
sns.countplot(x = 'age',  palette="mako", data = train)
ax.set_xlabel('Age Number', fontsize=20)
ax.set_ylabel('Count', fontsize=20)
ax.set_title('Distribution of Age', fontsize=20)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(14, 7)
sns.countplot(x = 'job',  palette="rocket", data = train)
ax.set_xlabel('Job Type', fontsize=17)
ax.set_ylabel('Count', fontsize=17)
ax.set_title('Job Distribution', fontsize=17)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(12, 7)
sns.countplot(x = 'marital',  palette="viridis", data = train)
ax.set_xlabel('Marital Status', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Status', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(12, 7)
sns.countplot(x = 'education',  palette="Blues", data = train)
ax.set_xlabel('Highest Level of Education', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Education Distribution', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 7)
sns.countplot(x = 'loan',  palette="icefire", data = train)
ax.set_xlabel('Loan Taken?', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Personal Loan', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 7)
sns.countplot(x = 'housing',  palette="flare", data = train)
ax.set_xlabel('Loan Taken?', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Housing Loan', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(12, 7)
sns.countplot(x = 'contact',  palette="magma", data = train)
ax.set_xlabel('Contact Type', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Point of Contact', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 7)
sns.countplot(x = 'month',  palette="viridis", data = train)
ax.set_xlabel('Month Contacted?', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Month', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 7)
sns.countplot(x = 'day_of_week',  palette="rocket", data = train)
ax.set_xlabel('Day Contacted?', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Day of Week', fontsize=28)
sns.despine()

sns.countplot(x='duration',data=train)

fig, ax = plt.subplots()
fig.set_size_inches(17, 8)
sns.countplot(x = 'campaign',  palette="crest", data = train)
ax.set_xlabel('Number of Contacts', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Campaign', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 8)
sns.countplot(x = 'pdays',  palette="rocket_r", data = train)
ax.set_xlabel('Days Passed', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Days Passed', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 8)
sns.countplot(x = 'poutcome',  palette="rocket", data = train)
ax.set_xlabel('Outcome', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Previous Outcome', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 8)
sns.countplot(x = 'nr.employed',  palette="magma", data = train)
ax.set_xlabel('Number of Employees', fontsize=28)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Employees', fontsize=28)
sns.despine()

fig, ax = plt.subplots()
fig.set_size_inches(17, 8)
sns.countplot(x = 'Subscribed',  palette="rocket_r", data = train)
ax.set_xlabel('Subscribed a Term Deposit?', fontsize=22)
ax.set_ylabel('Count', fontsize=28)
ax.set_title('Subscribed', fontsize=28)
sns.despine()

"""# **Data Preprocessing** used for data cleansing and deriving features from dataset

Data Cleaning
"""

#Changing subscribed from categorical to numerical
train['Subscribed_val'] = train['Subscribed']
train['Subscribed_val'] = train['Subscribed_val'].map({'yes': 1, 'no' :0})
test['Subscribed_val'] = test['Subscribed']
test['Subscribed_val'] = test['Subscribed_val'].map({'yes': 1, 'no' :0})

# replacing unknown values with employed for job column and single for marital column
train['job'] = train['job'].replace(['unknown'], 'unemployed')
train['marital'] = train['marital'].replace(['unknown'], 'single')
test['job'] = test['job'].replace(['unknown'], 'unemployed')
test['marital'] = test['marital'].replace(['unknown'], 'single')

#Label Encoding all the categorical features for train
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
labelencoder.fit(train['job'])
train['job'] = labelencoder.transform(train['job'])
labelencoder.fit(train['marital'])
train['marital'] = labelencoder.transform(train['marital'])
labelencoder.fit(train['poutcome'])
train['poutcome'] = labelencoder.transform(train['poutcome'])
labelencoder.fit(train['contact'])
train['contact'] = labelencoder.transform(train['contact'])
labelencoder.fit(train['month'])
train['month'] = labelencoder.transform(train['month'])
labelencoder.fit(train['day_of_week'])
train['day_of_week'] = labelencoder.transform(train['day_of_week'])
train

#Label Encoding all the categorical features for test
from sklearn.preprocessing import LabelEncoder
labelencoderr = LabelEncoder()
labelencoderr.fit(test['job'])
test['job'] = labelencoderr.transform(test['job'])
labelencoderr.fit(test['marital'])
test['marital'] = labelencoderr.transform(test['marital'])
labelencoderr.fit(test['poutcome'])
test['poutcome'] = labelencoderr.transform(test['poutcome'])
labelencoderr.fit(test['contact'])
test['contact'] = labelencoderr.transform(test['contact'])
labelencoderr.fit(test['month'])
test['month'] = labelencoderr.transform(test['month'])
labelencoderr.fit(test['day_of_week'])
test['day_of_week'] = labelencoderr.transform(test['day_of_week'])
test

# changing education from categorical to numerical using ordinal
train['education_val'] = train['education']
train['education_val'] = train['education_val'].map({'unknown': 0, 'illiterate': 0, 'basic.4y': 1, 'basic.6y': 2, 'basic.9y': 3, 'high.school': 4, 'professional.course': 5, 'university.degree': 6})
test['education_val'] = test['education']
test['education_val'] = test['education_val'].map({'unknown': 0, 'illiterate': 0, 'basic.4y': 1, 'basic.6y': 2, 'basic.9y': 3, 'high.school': 4, 'professional.course': 5, 'university.degree': 6})

# changing housing from categorical to numerical using ordinal
train['housing_val'] = train['housing']
train['housing_val'] = train['housing_val'].map({'yes': 1, 'no': 0, 'unknown': 0})
test['loan_val'] = test['loan']
test['loan_val'] = test['loan_val'].map({'yes': 1, 'no': 0, 'unknown': 0})

#Analyzing columns and rows
for var in train:
    print('*********************************************************')
    print('---------------------------')
    print(var)
    print('---------------------------')
    print(train[var].value_counts())

train.drop(['Subscribed', 'housing', 'loan', 'education'], axis=1, inplace=True)
test.drop(['Subscribed', 'housing', 'loan', 'education'], axis=1, inplace=True)

#displaying the data to see results
train

"""Feature Selection"""

#Using the updated heat map, we have decided to use 'age' and 'education_val' to create our decision tree'
plt.subplots(figsize=(12,12))
sns.heatmap(train.corr(), annot=True)
plt.show()

"""# **Modelling**

Decesion Tree Classifier
"""

# We created a decision tree
from sklearn import tree

tree1 = tree.DecisionTreeClassifier()
tree1 = tree1.fit(train[['age','education_val']], train['Subscribed_val'])
tree.plot_tree(tree1)

predicted_1 = tree1.predict(test[['age','education_val']])

#dislpay scores of precision, recall, f1-score, accuracy, support, macro avg, and weighted avg
from sklearn.metrics import classification_report

print(classification_report(test['Subscribed_val'].values.reshape((-1,1)), predicted_1,))

"""KNN Clasifier"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)

train_y = train['Subscribed_val']
# remove target class from input attributes
train_X = train.drop(labels='Subscribed_val', axis=1)
test_y = test['Subscribed_val']
# remove target class from input attributes
test_X = test.drop(labels='Subscribed_val', axis=1)

# train a KNN using all attributes (obviously except the target class!)
knn.fit(train_X, train_y)

y_predicted = knn.predict(test_X)

y_predicted

y_real = test['Subscribed_val'].values
y_real

print(classification_report(y_real,y_predicted))

accuracy = sum(y_real == y_predicted) / len(y_real)
print("the accuracy of this model is:", accuracy)

print('number of (1) in real_classes is:', sum(y_real))
print('number of (0) in real_classes is:', (len(y_real) - sum(y_real)))

from sklearn.metrics import confusion_matrix
confusion_matrix(y_predicted, y_real, labels=[1,0])

TP, FP, FN, TN = confusion_matrix(y_predicted, y_real, labels=[1,0]).ravel()
(TP, FP, FN, TN)

"""## **Conclusion**"""

#Decision Tree is a better model than KNN for this dataset